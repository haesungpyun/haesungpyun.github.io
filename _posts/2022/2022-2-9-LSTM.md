---
title: LSTM
layout: post
Created: February 09, 2022 4:40 PM
tags:
    - Road to Transformer
    - LSTM
comments: true
use_math: true
sitemap :
  changefreq : daily
  priority : 1.0
---

>LSTM - RNN의 gradient vanishing을 해결하기 위해서 가중치를 사용하는 gate를 도입했다

Transformer를 최종적으로 구현하기 위해 하나씩 공부하고 있다.


###LSTM

앞선 RNN 모델의 경우 너무 많은 cell을 갖게 되면, 즉 sequence의 길이가 길어지게 되면, gradient vanishing 현상이 나타나게 된다.

이것은 tanh 함수로 인해서 모든 값의 편 미분 값이 1보다 작아지게 되고, 몇 개의 cell로 구성돼 곱해지다보면, 0에 수렴하게 된다.

위의 문제를 해결하기 위해서 hidden state와 추가적인 cell state를 넘겨주게 된다.

이 cell state의 경우에는 함수에 직접적으로 들어가지 않아서 sequence가 길어져도 0으로 수렴하지 않을 수 있다.

LSTM도 cell로 구성된다.
LSTM에서 cell state, 입력 x, hidden state h를 적절히 고려하기 위해서 gate를 사용한다.

forget gate, input gate, output gate를 사용한다.

LSTM의 한 cell1은 입력으로 이전 cell의 cell state c0, 이전 cell의 hidden state h0, 입력 x1을 받는다.

+ forget gate
들어온 입력 h0와 x1을 고려한다. 이 두 입력을 고려하여 cell state에서 없애야 하는 정보를 선별한다. h0와 x1을 통해서 얻은 값을 시그모이드 함수에 넣으면 0-1 사이의 값이 나오게 된다. 이것을 가중치로 하여 이전 cell state c0에 곱한다. cell state에서 현재 입력 x1, 이전 cell의 hidden state h0를 기준으로 판단하기에 필요 없는 정보는 없어진다.

  >ft = Sigmoid(Wxf x1 + Whf h0 + b)

+ input gate
위의 forget gate와 동일한 식을 갖는다. 그러나 forget gate에서는 이전 cell state c0에서 없앨 정보를 선별하기 위해 시그모이드 함수를 사용한 것이라면 이번에는 현재 cell state c1  입력할 정보를 선별하기 위해 사용한다. 현재 cell state c1에 입력될 정보는 RNN과 마찬가지로 hidden state h0, input x1에 의해서 결정된다. x1과 h0를 RNN에서 hidden state를 만들 때와 같이 tanh()에 넣어서 h'1을 생성한다. input gate에서 나온 정보와 h'1을 곱하게 되면(element wise하게) h'1의 요소 별로 얼마나 고려를 할지 가중치를(input gate이 sigmoid에 들어갔다 나오면 0-1 값을 갖게 되니까) 고려하게 된다. 여기서 구한 h'1을 고려해 최종 h1을 생성한다.

   >it = Sigmoid(Wxi x1 + Whi h0 + b)
   >h't = tanh(Wxh' x1 + Whh' h0 + b)

+ cell sate
현재 x1과 h0를 고려하였을 때, 이전 cell state c0에서 삭제해야 하는 부분을 forget gate에서 나온 ft와 c0을 곱하여 구했다. 그 이후에 현재 cell1에서 x1과 h0를 고려해 기존 cell state c0가 추가적으로 알아야 하는 정보를 추가했다. 기억할 feature와 그 가중치를 input gate를 통해서 h't와 it를 곱하여(element wise하게) 구했다. 다음 cell2에 전달될 cell state는 이 두 정보를 더 구할 수 있다.

  >ct = ft * ct-1 + h't*it

+ output gate
  위의 gate들과 동일하게 x1과 h0를 고려하여 현재 hidden state를 구할 때 사용될 가중치를 구한다. h1을 구할 때에는 현재 cell state c1을 고려하여 구한다. 이 c1을 tanh에 넣어 -1-1 사이 값을 출력한다. 이 두 값을 곱하면 다음 cell, cell2에게 전달되거나 출력층으로 갈 hidden state h1을 생성할 수 있다.

  >ot = Sigmoid(Wx0 x1 + Wh0 h0 + b)
  >ht = ot * tanh(ct)

이 LSTM은 상당히 계산이 많고 복잡한 것을 알 수 있다. 또 얼핏 보기에도 동일한 연산이 중복돼 수행되는 것 같다. 그래서 나온 것이 GRU이다. parameter 수가 더 적고 연산이 간단하다. 그러나 LSTM이 더 많이 사용된다고 한다.
