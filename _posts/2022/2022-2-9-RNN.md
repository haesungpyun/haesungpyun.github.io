---
title: RNN
layout: post
Created: February 09, 2022 2:33 PM
tags:
    - Road to Transformer
    - RNN
comments: true
use_math: true
sitemap :
  changefreq : daily
  priority : 1.0
---

>RNN - 이전 cell의 hidden state와 입력을 계산해 hidden state를 다음 cell에 전달한다.


Transformer를 최종적으로 구현하기 위해 하나씩 공부해 나가고 있다.


### RNN

RNN은 여러 개의 cell로 구성된다. 이 cell의 개수는 주어지는 sequence의 길이에 따라 임의로 정할 수 있다.
그러나 너무 많이 사용하게 되면 gradient vanishing 현상이 발생한다.

RNN의 입, 출력 방식에 따라
one to one,
one to many(이미지에 caption 달기),
many to one(time series data 분류),
many to many(Translation)
로 구분 된다.

RNN을 구성하는 cell1는 입력 x1와 이전 cell, cell0에서 보낸 hidden state(h0)를 입력으로 받는다. 주어진 두 입력을 통해서 hidden state를 계산한다.

x1과 h0에 weight들을 곱해준다. Wxh @ x1과 Whh @ h0를 얻게 된다. Wxh와 Whh는 서로 다른 matrix이다. 이렇게 얻은 두 값을 element wise하게 더해준다.

이렇게 얻은 값을 tanh 함수에 넣어주 -1, 1 사이의 값으로 만들어 출력한다. 출력한 새로운 hidden state, h1은 그 다음 cell, cell2로 전파된다.

>h1 =  tanh(Wxh x1 + Whh h0 + b)

만약 cell 이후에 출력층 혹은 추가적인 layer가 있다면 위에서 얻은 h1을 활성화 함수에 넣거나 MLP에 넣거나 Sigmoid에 넣어 최종 값을 출력한다.

>y1 = f(Why h1 + b)

여기서 사용된 weight들(Wxh, Whh, Why)는 서로 다른 값이다. 그러나 이 행렬들은 cell에 상관 없이 같이 사용된다. cell0에서 사용하는 weight 값들과 cell1, cell2, ...에서 사용하는 weight 값이 같다. 같은 layer에 속해 있는 cell에서는 모두 weight를 공유한다. 이 layer가 업데이트 될 때 3가지의 weight가 업데이트된다.  
